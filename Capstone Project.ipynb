{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering Capstone Project - Metro_Madrid\n",
    "\n",
    "## Project Summary\n",
    "This notebook gets output data from [metro-big-data-unir](https://github.com/juananthony/metro-big-data-unir) project and create a database to analysis.\n",
    "\n",
    "[*Metro de Madrid*](https://www.metromadrid.es/) is the name of the tube/subway service that operates in Madrid, Spain. This service has 302 stations on 13 lines plus a light rail system called *Metro Ligero*. This service is used, on average in 2020, more than 30 million times each month.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('udacity-capstone').getOrCreate()\n",
    "spark.conf.set('spark.sql.session.timeZone', 'CET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "\n",
    "### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data'\n",
    "lines_file = 'lines.csv'\n",
    "stations_file = 'stations.csv'\n",
    "mentions_file = 'mentions_20210126.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = spark.read.csv(os.path.join(DATA_PATH, lines_file), header=True)\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations = spark.read.csv(os.path.join(DATA_PATH, stations_file), header=True)\n",
    "stations.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "946929"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions = spark.read.csv(os.path.join(DATA_PATH, mentions_file), header=True, multiLine=True, escape='\"')\n",
    "mentions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- in_reply_to_status_id: string (nullable = true)\n",
      " |-- user.id: string (nullable = true)\n",
      " |-- user.name: string (nullable = true)\n",
      " |-- user.screen_name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- profile_image_url: string (nullable = true)\n",
      " |-- profile_image_url_https: string (nullable = true)\n",
      " |-- extended_tweet.full_text: string (nullable = true)\n",
      " |-- classification.naive_bayes.result: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mentions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+------+\n",
      "|classification.naive_bayes.result| count|\n",
      "+---------------------------------+------+\n",
      "|                             null| 83101|\n",
      "|                          nothing|693429|\n",
      "|                        complaint|111098|\n",
      "|                            issue| 59301|\n",
      "+---------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mentions.groupBy('`classification.naive_bayes.result`').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explore and Assess the Data\n",
    "### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "classes = ['complaint', 'issue']\n",
    "mentions = mentions.filter(col('`classification.naive_bayes.result`').isin(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions = mentions.withColumn('dt', to_timestamp(mentions.created_at, 'E MMM d HH:m:s Z y').alias('dt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "mentions = mentions.withColumn('full_text', \n",
    "                               when(~col('`extended_tweet.full_text`').isNull(), col('`extended_tweet.full_text`'))\n",
    "                               .otherwise(col('text'))).drop('text','`extended_tweet.full_text`')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the Data Model\n",
    "### 3.1 Conceptual Data Model\n",
    "The data we want to store is all messages that inform about any issue or complaint in a line or a station even if one message inform about an issue that affect two different lines. That the reason why the fact table is the inform fact, that can be a complaint or an issue. One tweet can inform about an issue that affect two lines (i.e.: a closed station and all lines that stops there). In other words, one tweet generates one or many \"inform fact\" records.\n",
    "\n",
    "![fact-dimension diagram](./img/class_diagram.png \"Fact-Dimension Diagram\")\n",
    "\n",
    "### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    " \n",
    "#### Line Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|line_name|line_id|\n",
      "+---------+-------+\n",
      "|       L9|      0|\n",
      "|      ML3|      1|\n",
      "|       L8|      2|\n",
      "|      ML1|      3|\n",
      "|      ML2|      4|\n",
      "+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "lines_dim = lines.withColumn('line_id', monotonically_increasing_id()).withColumnRenamed('line','line_name')\n",
    "lines_dim.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Station Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-------+\n",
      "|station_id|    station_name|line_id|\n",
      "+----------+----------------+-------+\n",
      "|         0|   Paco de Lucía|      0|\n",
      "|         1|      Mirasierra|      0|\n",
      "|         2|    Herrera Oria|      0|\n",
      "|         3|Barrio del Pilar|      0|\n",
      "|         4|        Ventilla|      0|\n",
      "+----------+----------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stations_dim = stations.join(lines_dim, lines_dim.line_name == stations.line, 'inner').select(monotonically_increasing_id().alias('station_id'), stations.station.alias('station_name'), lines_dim.line_id)\n",
    "stations_dim.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|class_name|class_id|\n",
      "+----------+--------+\n",
      "| complaint|       1|\n",
      "|     issue|       2|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "class_df = mentions.select(col('`classification.naive_bayes.result`').alias('class_name')).distinct()\n",
    "w = Window.orderBy('class_name')\n",
    "\n",
    "class_df.withColumn('class_id', row_number().over(w)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping date dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+---+-------+----+------+-------+\n",
      "|year|quarter|month|day|weekday|hour|minute|date_id|\n",
      "+----+-------+-----+---+-------+----+------+-------+\n",
      "|2018|4      |11   |17 |7      |19  |42    |0      |\n",
      "|2018|4      |11   |20 |3      |11  |56    |1      |\n",
      "|2018|4      |11   |26 |2      |12  |22    |2      |\n",
      "|2018|4      |11   |30 |6      |0   |24    |3      |\n",
      "|2018|4      |11   |30 |6      |14  |35    |4      |\n",
      "|2018|4      |11   |30 |6      |19  |47    |5      |\n",
      "|2018|4      |11   |30 |6      |20  |44    |6      |\n",
      "|2018|4      |12   |1  |7      |9   |3     |7      |\n",
      "|2018|4      |12   |6  |5      |9   |50    |8      |\n",
      "|2018|4      |12   |6  |5      |10  |56    |9      |\n",
      "+----+-------+-----+---+-------+----+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, quarter, dayofweek, dayofmonth, hour, minute\n",
    "\n",
    "dates_df = mentions.select(\n",
    "                        year(col('dt')).alias('year'),\n",
    "                        quarter(col('dt')).alias('quarter'),\n",
    "                        month(col('dt')).alias('month'),\n",
    "                        dayofmonth(col('dt')).alias('day'),\n",
    "                        dayofweek(col('dt')).alias('weekday'),\n",
    "                        hour(col('dt')).alias('hour'),\n",
    "                        minute(col('dt')).alias('minute')) \\\n",
    "                .dropDuplicates() \\\n",
    "                .withColumn('date_id', monotonically_increasing_id())\n",
    "\n",
    "dates_df.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweet Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------+\n",
      "|tweet_id           |date_id     |user_id  |text                                                                                                                                                          |reply_tweet_id|\n",
      "+-------------------+------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------+\n",
      "|1062598007793860609|292057776270|289007361|@metro_madrid y ¿hoy que le pasa a la Línea 10 dirección Tres Olivos? ¿Por qué nos paramos en todas las estaciones? Todos los días una aventura! #máximapereza|null          |\n",
      "+-------------------+------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import substring\n",
    "\n",
    "mentions.join(dates_df,\n",
    "              (year(mentions.dt) == dates_df.year) &\n",
    "              (month(mentions.dt) == dates_df.month) &\n",
    "              (dayofmonth(mentions.dt) == dates_df.day) &\n",
    "              (hour(mentions.dt) == dates_df.hour) &\n",
    "              (minute(mentions.dt) == dates_df.minute),\n",
    "              'inner'\n",
    "             ) \\\n",
    "        .select(col('id').alias('tweet_id'),\n",
    "                'date_id',\n",
    "                col('`user.id`').alias('user_id'),\n",
    "                col('full_text').alias('text'),\n",
    "                col('in_reply_to_status_id').alias('reply_tweet_id')\n",
    "               ).show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+------------------------+\n",
      "|_id                               |substring(_id, 10, 24)  |\n",
      "+----------------------------------+------------------------+\n",
      "|ObjectId(5be9f6d0fab7400004ae89c8)|5be9f6d0fab7400004ae89c8|\n",
      "|ObjectId(5be9ee7520a224000468a64a)|5be9ee7520a224000468a64a|\n",
      "|ObjectId(5bea7501fab7400004ae89e0)|5bea7501fab7400004ae89e0|\n",
      "|ObjectId(5bea6dd0fab7400004ae89d4)|5bea6dd0fab7400004ae89d4|\n",
      "|ObjectId(5bea75c5fab7400004ae89e6)|5bea75c5fab7400004ae89e6|\n",
      "|ObjectId(5bea08b5212c7f0004857734)|5bea08b5212c7f0004857734|\n",
      "|ObjectId(5bea7258fab7400004ae89da)|5bea7258fab7400004ae89da|\n",
      "|ObjectId(5bea1256212c7f000485799f)|5bea1256212c7f000485799f|\n",
      "|ObjectId(5bea1614212c7f0004857aaa)|5bea1614212c7f0004857aaa|\n",
      "|ObjectId(5bea7a59fab7400004ae8a00)|5bea7a59fab7400004ae8a00|\n",
      "|ObjectId(5bea139a212c7f00048579eb)|5bea139a212c7f00048579eb|\n",
      "|ObjectId(5bea766bfab7400004ae89ec)|5bea766bfab7400004ae89ec|\n",
      "|ObjectId(5beaad42212c7f00048599d4)|5beaad42212c7f00048599d4|\n",
      "|ObjectId(5beab280fab7400004ae8a68)|5beab280fab7400004ae8a68|\n",
      "|ObjectId(5beaa875fab7400004ae8a5a)|5beaa875fab7400004ae8a5a|\n",
      "|ObjectId(5bea75e4fab7400004ae89e8)|5bea75e4fab7400004ae89e8|\n",
      "|ObjectId(5beabe8efab7400004ae8a7c)|5beabe8efab7400004ae8a7c|\n",
      "|ObjectId(5beacd80fab7400004ae8a8e)|5beacd80fab7400004ae8a8e|\n",
      "|ObjectId(5beac1e5fab7400004ae8a82)|5beac1e5fab7400004ae8a82|\n",
      "|ObjectId(5beab59dfab7400004ae8a6c)|5beab59dfab7400004ae8a6c|\n",
      "+----------------------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mentions.select('_id', substring(col('_id'), 10, 24)).show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Pipelines to Model the Data \n",
    "### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
