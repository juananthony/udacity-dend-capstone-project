{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering Capstone Project - Metro_Madrid <a class=\"anchor\" id=\"top\"></a>\n",
    "\n",
    "## Project Summary\n",
    "This notebook gets output data from [metro-big-data-unir](https://github.com/juananthony/metro-big-data-unir) project and create a model for a data lake. The used data is based on all mentions on Twitter to offcial account of Metro de Madrid service.\n",
    "\n",
    "[*Metro de Madrid*](https://www.metromadrid.es/) is the name of the tube/subway service that operates in Madrid, Spain. This service has 302 stations on 13 lines plus a light rail system called *Metro Ligero*. This service was used in 2019 more than 677 million times.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* [Step 1: Scope the Project and Gather Data](#step-1)\n",
    "* [Step 2: Explore and Assess the Data](#step-2)\n",
    "* [Step 3: Define the Data Model](#step-3)\n",
    "* [Step 4: Run ETL to Model the Data](#step-4)\n",
    "* [Step 5: Project Write Up](#step-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Set up the environment\n",
    "First of all, a Spark session is needed to process all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "                .builder \\\n",
    "                .appName('udacity-capstone') \\\n",
    "                .master(\"local[*]\") \\\n",
    "                .getOrCreate()\n",
    "spark.conf.set('spark.sql.session.timeZone', 'CET')\n",
    "spark.conf.set('spark.driver.memory', '16g')\n",
    "spark.conf.set('spark.executor.memory', '16g')\n",
    "spark.conf.set('spark.executor.extraJavaOptions', '-Xms1024 -Xmx8g -XX:+UseParallelGC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "[Back to top](#top)\n",
    "## Step 1: Scope the Project and Gather Data <a class=\"anchor\" id=\"step-1\"></a>\n",
    "\n",
    "### Scope \n",
    "[metro-big-data-unir](https://github.com/juananthony/metro-big-data-unir) project collects tweets from Twitter. Tweets which [@metro_madrid](https://twitter.com/metro_madrid) account is mentioned. Many messages in this social network used to be complaints about the service quality (broken mechanic stairs or air conditionar, ...), other tweets mentioned some issue in the service (huge delays in a line, ...). This project uses NLP techniques to detect those messages where an issue or complaint is mentioned to [@metro_madrid](https://twitter.com/metro_madrid) account and store all of them in a MongoDB database.\n",
    "\n",
    "This notebook uses a CSV export of data stored in MongoDB instance. All this messages are classfied previously by [metro-big-data-unir](https://github.com/juananthony/metro-big-data-unir).\n",
    "\n",
    "### Describe and Gather Data \n",
    "\n",
    "#### Dataset files\n",
    "First, it is needed to define where the files are stored.\n",
    "\n",
    "**Important**: Mentions file needs to be extracted from ```mentions_20210210.7z``` file. This file is greated than the 100Mb Github limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data'\n",
    "lines_file = 'lines.csv'\n",
    "stations_file = 'stations.csv'\n",
    "mentions_file = 'mentions_20210210.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 3 file to use:\n",
    "* ```lines_file```: It contains information about the lines offered by Metro de Madrid service.\n",
    "* ```stations_file```: This file contains information about the stations.\n",
    "* ```mentions_file```: It has all tweets where [@metro_madrid](https://twitter.com/metro_madrid) was mentioned.\n",
    "\n",
    "#### Lines dataset\n",
    "This dataset contains information about all lines in metro de Madrid service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = spark.read.csv(os.path.join(DATA_PATH, lines_file), header=True).dropDuplicates()\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- line: string (nullable = true)\n",
      " |-- regex: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 16 lines in the service and has 2 fields, the line name and a regex expression to search that line in a text.\n",
    "\n",
    "#### Stations dataset\n",
    "This file contains information about all stations in metro de madrid service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "281"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations = spark.read.csv(os.path.join(DATA_PATH, stations_file), header=True).dropDuplicates()\n",
    "stations.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station: string (nullable = true)\n",
      " |-- regex: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stations.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 281 stations in the service and has 2 fields also, the station name and a regex expression to search that station in a text.\n",
    "\n",
    "#### Mentions dataset\n",
    "This file is a MongoDB extraction as CSV file of all tweets stored previously. To read that CSV, a schema is used to ensure the Spark reading process uses the proper data types for each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "955167"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from schemas.mentions_schema import mentions_schema\n",
    "from pyspark import StorageLevel\n",
    "mentions = spark \\\n",
    "                .read \\\n",
    "                .csv(os.path.join(DATA_PATH, mentions_file), header=True, multiLine=True, escape='\"', schema=mentions_schema)\n",
    "mentions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|classification| count|\n",
      "+--------------+------+\n",
      "|          null| 85465|\n",
      "|       nothing|698591|\n",
      "|     complaint|111612|\n",
      "|         issue| 59499|\n",
      "+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mentions.groupBy('classification').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- in_reply_to_status_id: string (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- user_screen_name: string (nullable = true)\n",
      " |-- user_description: string (nullable = true)\n",
      " |-- user_profile_image_url: string (nullable = true)\n",
      " |-- user_profile_image_url_https: string (nullable = true)\n",
      " |-- extended_tweet_full_text: string (nullable = true)\n",
      " |-- classification: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mentions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains 902,865 records and are classified in 4 different values: ```null```, ```nothing```, ```complaint```, and ```issue```. It has the following fields:\n",
    "* ```_id```: \n",
    "    * ```string``` value.\n",
    "    * MongoDB document identifier.\n",
    "* ```created_at```:\n",
    "    * ```string``` value.\n",
    "    * This field represent the exact time where the tweet message was created. It is formatted using ```E MMM d HH:m:s Z y``` format.\n",
    "* ```text```:\n",
    "    * ```string``` value.\n",
    "    * Tweet text. This field has 140 character. For that reason, it can be truncated if the text has more than 140. If that happen, the complete text is stored in another field: ```extended_tweet_full_text```.\n",
    "* ```id```:\n",
    "    * ```long``` value.\n",
    "    * Tweet id. This id is a unique value for each tweet message.\n",
    "* ```in_reply_to_status_id```:\n",
    "    * ```string``` value.\n",
    "    * In Twitter, users can post a tweet replying to another tweet. If a user replies to another one, the other tweet id is stored in this field.\n",
    "* ```user_id```:\n",
    "    * ```long``` value.\n",
    "    * This id is a unique value for users. This value contains the id of the tweet author.\n",
    "* ```user_name```:\n",
    "    * ```string``` value.\n",
    "    * Full name of the user.\n",
    "* ```user_screen_name```:\n",
    "    * ```string``` value.\n",
    "    * This is an unique user nickname.\n",
    "* ```user_description```:\n",
    "    * ```string``` value.\n",
    "    * This is the description that every user write about themselves.\n",
    "* ```user_profile_image_url```:\n",
    "    * ```string``` value.\n",
    "    * URL to get the user profile image.\n",
    "* ```user_profile_image_url_https```:\n",
    "    * ```string``` value.\n",
    "    * URL to get the user profile image (using HTTPS).\n",
    "* ```extended_tweet_full_text```:\n",
    "    * ```string``` value.\n",
    "    * If ```text``` field is truncated because it is longer than 140 characters, ```extended_tweet_full_text``` stored the entire tweet text.\n",
    "* ```classification```:\n",
    "    * ```string``` value.\n",
    "    * This field stores the classification result by [metro-big-data-unir](https://github.com/juananthony/metro-big-data-unir)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "[Back to top](#top)\n",
    "## Step 2: Explore and Assess the Data <a class=\"anchor\" id=\"step-2\"></a>\n",
    "### Explore the Data \n",
    "The goal of this step is to identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Lines\n",
    "Check null if any line row exists with ```line``` or ```regex``` with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "lines.filter(col('line').isNull() | col('regex').isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there is any station row contains null values in any column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations.filter(col('station').isNull() | col('regex').isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it checks if there is any duplicate by line name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|line|count|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines.groupBy('line').count().filter(col('count') > 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It checks if there is any duplicate by station name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|station|count|\n",
      "+-------+-----+\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stations.groupBy('station').count().filter(col('count') > 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it ensures that there is any mention duplicated by tweet id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52284"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "mentions.groupBy('id').count().filter(col('count')>1).orderBy(desc('count')).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is tweet duplicates. It can be store several times because of different Twitter listeners working at the same time. In the next steps, those duplicates will be removed.\n",
    "\n",
    "The next cell checks if there is any mention row with null values in the following fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----+---+---------------------+-------+---------+----------------+----------------+----------------------+----------------------------+------------------------+--------------+\n",
      "|_id|created_at|text| id|in_reply_to_status_id|user_id|user_name|user_screen_name|user_description|user_profile_image_url|user_profile_image_url_https|extended_tweet_full_text|classification|\n",
      "+---+----------+----+---+---------------------+-------+---------+----------------+----------------+----------------------+----------------------------+------------------------+--------------+\n",
      "+---+----------+----+---+---------------------+-------+---------+----------------+----------------+----------------------+----------------------------+------------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mentions.filter(col('id').isNull()\n",
    "                | col('text').isNull()\n",
    "                | col('user_id').isNull()\n",
    "                | col('user_screen_name').isNull()\n",
    "                | col('created_at').isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Steps\n",
    "It is need to clean the data to remove duplicate rows and to transform, filter useless data and transform datetime format.\n",
    "\n",
    "First, it is need to remove duplicates in mentions dataframe based on ```id``` and ````text``` fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions = mentions \\\n",
    "                .dropDuplicates(['id', 'text']) \\\n",
    "                .persist(StorageLevel.MEMORY_ONLY_SER)\n",
    "\n",
    "mentions.groupBy('id').count().filter(col('count')>1).orderBy(desc('count')).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we check how many records we have without duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902865"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we want a datalake about issues and complaints tweets and once the duplicates are removed, we need to filter the data by classification value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "classes = ['complaint', 'issue']\n",
    "mentions = mentions.filter(col('classification').isin(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we need to create a new field called ```dt``` formatting string ```created_at``` into a timestamp field. To convert that we need to use the correct datetime format that ```create_at``` uses: ```E MMM d HH:m:s Z y```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions = mentions.withColumn('dt', to_timestamp(mentions.created_at, 'E MMM d HH:m:s Z y').alias('dt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, due to the cauistry of the ```text``` field, a new column ```full_text``` must be created to use ```extended_tweet_full_text``` when it is not null, otherwise, it uses ```text```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "mentions = mentions.withColumn('full_text', \n",
    "                               when(~col('`extended_tweet_full_text`').isNull(), col('`extended_tweet_full_text`'))\n",
    "                               .otherwise(col('text'))).drop('text','`extended_tweet_full_text`') \\\n",
    "                               .persist(StorageLevel.MEMORY_ONLY_SER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "[Back to top](#top)\n",
    "## Step 3: Define the Data Model  <a class=\"anchor\" id=\"step-3\"></a>\n",
    "### 3.1 Conceptual Data Model\n",
    "The data we want to store is all messages that inform about any issue or complaint in a line or a station even if one message inform about an issue that affect two different lines. That the reason why the fact table is the inform fact, that can be a complaint or an issue. One tweet can inform about an issue that affect two lines (i.e.: a closed station and all lines that stops there). In other words, one tweet generates one or many \"inform fact\" records.\n",
    "\n",
    "![fact-dimension diagram](./img/class_diagram.png \"Fact-Dimension Diagram\")\n",
    "\n",
    "### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    " \n",
    "#### Line Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "[Back to top](#top)\n",
    "## Step 4: Run ETL to model the data<a class=\"anchor\" id=\"step-4\"></a>\n",
    "\n",
    "### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"./out\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line Dimension\n",
    "This dimension is based on the file content with all lines of Metro Madrid service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "lines_w = Window.orderBy('line_name')\n",
    "lines_dim = lines.withColumnRenamed('line','line_name').withColumn('line_id', row_number().over(lines_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lines dimension is persisted as parquet file in ```lines``` folder inside the ```OUTPUT_DIR``` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_file = os.path.join(OUTPUT_DIR, 'lines')\n",
    "lines_dim.write.mode('overwrite').parquet(lines_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Station Dimension\n",
    "Station dimensino is based on the file content with all stations of Metro de Madrid service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_w = Window.orderBy('station_name')\n",
    "stations_dim = stations.select(stations.station.alias('station_name'), 'regex').withColumn('station_id', row_number().over(station_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stations dimension is persisted as parquet file in ```stations``` folder inside the ```OUTPUT_DIR``` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_file = os.path.join(OUTPUT_DIR, 'stations')\n",
    "stations_dim.write.mode('overwrite').parquet(stations_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Class dimension\n",
    "This dimension contains all possible incident that can be detected on tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_df = mentions.select(col('classification').alias('class_name')).distinct()\n",
    "class_w = Window.orderBy('class_name')\n",
    "class_dim = class_df.withColumn('class_id', row_number().over(class_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class dimension is persisted as parquet file in ```classes``` folder inside the ```OUTPUT_DIR``` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_file = os.path.join(OUTPUT_DIR, 'classes')\n",
    "class_dim.write.mode('overwrite').parquet(class_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Date dimension\n",
    "This dimension is based on all date entries in tweet mentions. The ```create_at``` datetime is splitted in the following fields: ```year```, ```month```, ```day```, ```hour```, ```minute```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, quarter, dayofweek, dayofmonth, hour, minute\n",
    "\n",
    "date_w = Window.orderBy('year', 'month', 'day', 'hour', 'minute')\n",
    "\n",
    "date_dim = mentions.select(\n",
    "                        year(col('dt')).alias('year'),\n",
    "                        quarter(col('dt')).alias('quarter'),\n",
    "                        month(col('dt')).alias('month'),\n",
    "                        dayofmonth(col('dt')).alias('day'),\n",
    "                        dayofweek(col('dt')).alias('weekday'),\n",
    "                        hour(col('dt')).alias('hour'),\n",
    "                        minute(col('dt')).alias('minute')) \\\n",
    "                .dropDuplicates() \\\n",
    "                .withColumn('date_id', row_number().over(date_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date dimension is persisted as parquet file in ```date``` folder inside the ```OUTPUT_DIR``` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_file = os.path.join(OUTPUT_DIR, 'date')\n",
    "date_dim.write.mode('overwrite').parquet(date_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### User Dimension\n",
    "This step selects the columns and rename them:\n",
    "* ```user.id``` -> ```user_id```\n",
    "* ```user.name``` -> ```user_name```\n",
    "* ```user.screen_name``` -> ```screen_name```\n",
    "* ```user.description``` -> ```description```\n",
    "* ```user.profile_image_url``` -> ```profile_image_url```\n",
    "* ```user.profile_image_url_https``` -> ```profile_image_url_https```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dim = mentions.select(\n",
    "            'user_id',\n",
    "            'user_name',\n",
    "            col('user_screen_name').alias('screen_name'),\n",
    "            col('user_description').alias('description'),\n",
    "            col('user_profile_image_url').alias('profile_image_url'),\n",
    "            col('user_profile_image_url_https').alias('profile_image_url_https')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User dimension is persisted as parquet file in ```users``` folder inside the ```OUTPUT_DIR``` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_file = os.path.join(OUTPUT_DIR, 'users')\n",
    "user_dim.write.mode('overwrite').parquet(users_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Tweet Dimension\n",
    "\n",
    "To model this dimension, is join to the ```date``` dimension by year, month, day, hour and minute and get the following fields:\n",
    "* tweet_id\n",
    "* date_id\n",
    "* user_id\n",
    "* text\n",
    "* reply_tweet_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring\n",
    "\n",
    "tweet_dim = mentions.join(date_dim,\n",
    "              (year(mentions.dt) == date_dim.year) &\n",
    "              (month(mentions.dt) == date_dim.month) &\n",
    "              (dayofmonth(mentions.dt) == date_dim.day) &\n",
    "              (hour(mentions.dt) == date_dim.hour) &\n",
    "              (minute(mentions.dt) == date_dim.minute),\n",
    "              'inner'\n",
    "             ) \\\n",
    "        .select(col('id').alias('tweet_id'),\n",
    "                'date_id',\n",
    "                col('`user_id`').alias('user_id'),\n",
    "                col('full_text').alias('text'),\n",
    "                col('in_reply_to_status_id').alias('reply_tweet_id')\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweet dimension is persisted as parquet file in ```tweets``` folder inside the ```OUTPUT_DIR``` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_file = os.path.join(OUTPUT_DIR, 'tweets')\n",
    "tweet_dim.coalesce(2).write.mode('overwrite').parquet(tweets_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Fact table\n",
    "In the tweet text any station or line can be mentioned. So, we used the regex included in lines and stations dataset to search those lines and stations in tweet text.\n",
    "\n",
    "First, we need to extract two dictionaries with ids and regex for lines and stations. Then, we search every line and station regex in all tweet text, creating two new columns with an array that contains the ids of lines/stations mentioned in it.\n",
    "\n",
    "<img src=\"./img/regex_web.png\" style=\"height:450px;margin-left:auto;margin-right:auto;border:1px solid #888;\"/>\n",
    "\n",
    "First of all, 3 methods are defined:\n",
    "* ```gen_tags()```. This method returns an array with tags. This tags are based on the dictionary with regex and the given text.\n",
    "    If any regex is satisfied for the given text, the key of the dictionary is appended to the tag array.\n",
    "* ```gen_line_tags()```. This method returns the an array tag with all line_id found in the given text.\n",
    "* ```gen_station_tags()```. This method returns a tag array with all station_id found in the given text.\n",
    "\n",
    "Once those methods are defined, they are used to create 2 ```udf```:\n",
    "* ```gen_line_tags_udf```\n",
    "* ```gen_station_tags_udf```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import re\n",
    "\n",
    "lines_dict = {elem.line_id: elem.regex for elem in lines_dim.collect()}\n",
    "stations_dict = {elem.station_id: elem.regex for elem in stations_dim.collect()}\n",
    "\n",
    "def gen_tags(text,dicc):\n",
    "    \"\"\"\n",
    "    Returns an array with tags. This tags are based on the dictionary with regex and the given text.\n",
    "    If any regex is satisfied for the given text, the key of the dictionary is appended to the tag array.\n",
    "    \"\"\"\n",
    "    tags = []\n",
    "    for key, expr in dicc.items():\n",
    "        if re.search(expr, text, re.IGNORECASE):\n",
    "            tags.append(key)\n",
    "    return tags\n",
    "\n",
    "def gen_line_tags(text):\n",
    "    \"\"\"\n",
    "    Returns an tag array with all line_id found in the given text.\n",
    "    \"\"\"\n",
    "    return gen_tags(text, lines_dict)\n",
    "    \n",
    "def gen_station_tags(text):\n",
    "    \"\"\"\n",
    "    Returns a tag array with all station_id found in the given text.\n",
    "    \"\"\"\n",
    "    return gen_tags(text, stations_dict)\n",
    "\n",
    "gen_line_tags_udf = udf(gen_line_tags, ArrayType(StringType()))\n",
    "gen_station_tags_udf = udf(gen_station_tags, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those ```udf``` functions are used to generate to new array columns: ```lines``` constains all line_id founded in ```full_text``` column using ```gen_line_tags_udf```, ```stations``` performs ```gen_station_tags_udf``` to generate all station_id founded in ```full_text```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_aux = mentions.withColumn('lines', gen_line_tags_udf(col('full_text'))) \\\n",
    "                     .withColumn('stations', gen_station_tags_udf(col('full_text'))) \\\n",
    "                        .persist(StorageLevel.MEMORY_ONLY_SER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fact table has two array columns, we join to class dimentions to get class_id. After that, it does an ```explode_outer``` to get one row per array entry (in both columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode_outer\n",
    "\n",
    "fact_w = Window.orderBy('id')\n",
    "\n",
    "fact_df = fact_aux \\\n",
    "        .join(class_dim, col('classification') == class_dim.class_name, 'inner') \\\n",
    "        .withColumn('issue_id', row_number().over(fact_w)) \\\n",
    "        .select('issue_id', col('id').alias('tweet_id'), 'class_id', 'lines', 'stations') \\\n",
    "        .select('issue_id', 'tweet_id', 'class_id', explode_outer('lines').alias('line_id'), 'stations') \\\n",
    "        .select('issue_id', 'tweet_id', 'class_id', 'line_id', explode_outer('stations').alias('station_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fact table is persisted as parquet file in ```fact_table``` folder inside the ```OUTPUT_DIR``` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_file = os.path.join(OUTPUT_DIR, 'fact_table')\n",
    "fact_df.coalesce(4).write.mode('overwrite').parquet(fact_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Data Quality Checks\n",
    "\n",
    "First, it reads the data persisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_stored = spark.read.parquet(date_file)\n",
    "user_stored = spark.read.parquet(users_file)\n",
    "class_stored = spark.read.parquet(class_file)\n",
    "tweet_stored = spark.read.parquet(tweets_file)\n",
    "station_stored = spark.read.parquet(stations_file)\n",
    "line_stored = spark.read.parquet(lines_file)\n",
    "fact_stored = spark.read.parquet(fact_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step checks that all dimensions hasn't null ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of null values in Date Dimension: 0\n",
      "# of null values in User Dimension: 0\n",
      "# of null values in Class Dimension: 0\n",
      "# of null values in Tweet Dimension: 0\n",
      "# of null values in Station Dimension: 0\n",
      "# of null values in Line Dimension: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"# of null values in Date Dimension: {date_stored.filter(col('date_id').isNull()).count()}\")\n",
    "print(f\"# of null values in User Dimension: {user_stored.filter(col('user_id').isNull()).count()}\")\n",
    "print(f\"# of null values in Class Dimension: {class_stored.filter(col('class_id').isNull()).count()}\")\n",
    "print(f\"# of null values in Tweet Dimension: {tweet_stored.filter(col('tweet_id').isNull() | col('user_id').isNull() | col('date_id').isNull()).count()}\")\n",
    "print(f\"# of null values in Station Dimension: {station_stored.filter(col('station_id').isNull()).count()}\")\n",
    "print(f\"# of null values in Line Dimension: {line_stored.filter(col('line_id').isNull()).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All facts must contains ```issue_id```, ```tweet_id``` and ```class_id```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_stored.filter(col('issue_id').isNull() | col('tweet_id').isNull() | col('class_id').isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is a unit text for ```gen_tags``` method. It checks the correct behaviour of this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIVEN \n",
      "\ta dictionary with regex expression: {'L1': 'l(i|í)neas? ?(^|\\\\D)1(\\\\D|$)| L ?(^|\\\\D)1(\\\\D|$)', 'L6': 'l(i|í)neas? ?(^|\\\\D)6(\\\\D|$)| L ?(^|\\\\D)6(\\\\D|$)'}\n",
      "\tand a given text: @metro_madrid informar de que hay un hombre borracho, línea 6 dirección ciudad universitaria vagón s_8495 ocupa todo los asientos. gracias.\n",
      "\n",
      "WHEN\n",
      "\tgen_tags('@metro_madrid informar de que hay un hombre borracho, línea 6 dirección ciudad universitaria vagón s_8495 ocupa todo los asientos. gracias.', '{'L1': 'l(i|í)neas? ?(^|\\\\D)1(\\\\D|$)| L ?(^|\\\\D)1(\\\\D|$)', 'L6': 'l(i|í)neas? ?(^|\\\\D)6(\\\\D|$)| L ?(^|\\\\D)6(\\\\D|$)'}')\n",
      "\n",
      "THEN\n",
      "\tresult should be equal to ['L6']: True\n"
     ]
    }
   ],
   "source": [
    "dicc_test = {\n",
    "    'L1': 'l(i|í)neas? ?(^|\\D)1(\\D|$)| L ?(^|\\D)1(\\D|$)',\n",
    "    'L6': 'l(i|í)neas? ?(^|\\D)6(\\D|$)| L ?(^|\\D)6(\\D|$)'\n",
    "}\n",
    "text_test = '@metro_madrid informar de que hay un hombre borracho, línea 6 dirección ciudad universitaria vagón s_8495 ocupa todo los asientos. gracias.'\n",
    "expected = ['L6']\n",
    "print(f\"GIVEN \\n\\ta dictionary with regex expression: {dicc_test}\\n\\tand a given text: {text_test}\\n\\nWHEN\\n\\tgen_tags(\\'{text_test}\\', \\'{dicc_test}\\')\\n\\nTHEN\\n\\tresult should be equal to {expected}: {gen_tags(text_test, dicc_test) == expected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Data dictionary \n",
    "\n",
    "#### Dimension Tables\n",
    "\n",
    "* **Line**\n",
    "    * ```line_id```\n",
    "        * ```Integer```\n",
    "        * Line identifier.\n",
    "    * ```line_name```\n",
    "        * ```String```\n",
    "        * Line name.\n",
    "    * ```regex```\n",
    "        * ```String```\n",
    "        * Regex to search the line in a text.\n",
    "* **Station**\n",
    "    * ```Integer```\n",
    "        * ```Long```\n",
    "        * Station identifier.\n",
    "    * ```station_name```\n",
    "        * ```String```\n",
    "        * Station name.\n",
    "    * ```regex```\n",
    "        * ```String```\n",
    "        * Regex to search the station in a text.\n",
    "* **Class**\n",
    "    * ```class_id```\n",
    "        * ```Integer```\n",
    "        * Station identifier.\n",
    "    * ```class_name```\n",
    "        * ```String```\n",
    "        * Station identifier.\n",
    "* **User**\n",
    "    * ```user_id```\n",
    "        * ```Long```\n",
    "        * User identifier.\n",
    "    * ```user_name```\n",
    "        * ```String```\n",
    "        * User name.\n",
    "    * ```screen_name```\n",
    "        * ```String```\n",
    "        * User unique string identifier.\n",
    "    * ```description```\n",
    "        * ```String```\n",
    "        * User description.\n",
    "    * ```profile_image_url```\n",
    "        * ```String```\n",
    "        * URL of profile image (HTTP protocol).\n",
    "    * ```profile_image_url_https```\n",
    "        * ```String```\n",
    "        * URL of profile image (HTTPS protocol).\n",
    "* **Date**\n",
    "    * ```date_id```\n",
    "        * ```Long```\n",
    "        * Date identifier.\n",
    "    * ```year```\n",
    "        * ```Integer```\n",
    "        * Year number (i.e.: 2019, 2020, 2021, ...).\n",
    "    * ```quarter```\n",
    "        * ```Integer```\n",
    "        * Quarter of the year (i.e.: 1, 2, ...).\n",
    "    * ```month```\n",
    "        * ```Integer```\n",
    "        * Month of the year as integer (i.e.: 1, 2, 3, 4, ...).\n",
    "    * ```weekday```\n",
    "        * ```Integer```\n",
    "        * Day of the week (Sunday=1, Monday=2, ..., Saturday=7).\n",
    "    * ```day```\n",
    "        * ```Integer```\n",
    "        * Day of the month (1, 2, 3, ...).\n",
    "    * ```hour```\n",
    "        * ```Integer```\n",
    "        * Hour in 24-hour format (i.e.: 0, 1, 2, ..., 12, 13, 14, ..., 22, 23).\n",
    "    * ```minute```\n",
    "        * ```Integer```\n",
    "        * Minute (i.e.: 0, 1, 2, 3, 4, ..., 59)\n",
    "* **Tweet**\n",
    "    * ```tweet_id```\n",
    "        * ```Long```\n",
    "        * Tweet identifier.\n",
    "    * ```date_id```\n",
    "        * ```Long```\n",
    "        * Date id when the tweet was created.\n",
    "    * ```user_id```\n",
    "        * ```Long```\n",
    "        * User id who is author of this tweet.\n",
    "    * ```text```\n",
    "        * ```String```\n",
    "        * Tweet text.\n",
    "    * ```reply_tweet_id```\n",
    "        * ```Long```\n",
    "        * If this tweet is a reply, this field references the tweet_id that this tweet is replying.\n",
    "\n",
    "#### Fact Table\n",
    "\n",
    "* **Inform**\n",
    "    * ```issue_id```\n",
    "        * ```Long```\n",
    "        * Inform identifier.\n",
    "    * ```tweet_id```\n",
    "        * ```Long```\n",
    "        * Tweet id that informs about a issue or complaint.\n",
    "    * ```line_id```\n",
    "        * ```Long```\n",
    "        * Service line id.\n",
    "    * ```station_id```\n",
    "        * ```Long```\n",
    "        * Station id.\n",
    "    * ```class_id```\n",
    "        * ```Long```\n",
    "        * If this tweet is a reply, this field references the tweet_id that this tweet is replying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "[Back to top](#top)\n",
    "### Step 5: Project Write Up <a class=\"anchor\" id=\"step-5\"></a>\n",
    "\n",
    "This project uses [Apache Spark](https://spark.apache.org/) to process large amount of date. This dataset is not quite large yet but it grows every day. In the future, other tube services from other cities can be integrated in the system and increase the number of tweets to process.\n",
    "\n",
    "When the record number increase by 100x, more executors need to be added. Also, in order to have a dashboard feeded by data and updated daily or hourly, [Apache Airflow](https://airflow.apache.org/) is needed to orchestrate MongoDB extraction, transformation and cleaning steps, etc.\n",
    "\n",
    "Now, the database is stored in parquet files. But when it is accessed by 100+ people, the data must be migrated to another system (Redshift, HDFS, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
