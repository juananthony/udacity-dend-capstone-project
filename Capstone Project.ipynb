{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering Capstone Project - Metro_Madrid <a class=\"anchor\" id=\"top\"></a>\n",
    "\n",
    "## Project Summary\n",
    "This notebook gets output data from [metro-big-data-unir](https://github.com/juananthony/metro-big-data-unir) project and create a database to analysis.\n",
    "\n",
    "[*Metro de Madrid*](https://www.metromadrid.es/) is the name of the tube/subway service that operates in Madrid, Spain. This service has 302 stations on 13 lines plus a light rail system called *Metro Ligero*. This service is used, on average in 2020, more than 30 million times each month.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* [Step 1: Scope the Project and Gather Data](#step-1)\n",
    "* [Step 2: Explore and Assess the Data](#step-2)\n",
    "* [Step 3: Define the Data Model](#step-3)\n",
    "* [Step 4: Run ETL to Model the Data](#step-4)\n",
    "* [Step 5: Project Write Up](#step-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Set up the environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "                .builder \\\n",
    "                .appName('udacity-capstone') \\\n",
    "                .master(\"local[*]\") \\\n",
    "                .getOrCreate()\n",
    "spark.conf.set('spark.sql.session.timeZone', 'CET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "[Back to top](#top)\n",
    "## Step 1: Scope the Project and Gather Data <a class=\"anchor\" id=\"step-1\"></a>\n",
    "\n",
    "### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data'\n",
    "lines_file = 'lines.csv'\n",
    "stations_file = 'stations.csv'\n",
    "mentions_file = 'mentions_20210210.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = spark.read.csv(os.path.join(DATA_PATH, lines_file), header=True).dropDuplicates()\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "281"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations = spark.read.csv(os.path.join(DATA_PATH, stations_file), header=True).dropDuplicates()\n",
    "stations.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from schemas.mentions_schema import mentions_schema\n",
    "mentions = spark.read.csv(os.path.join(DATA_PATH, mentions_file), header=True, multiLine=True, escape='\"', schema=mentions_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|classification| count|\n",
      "+--------------+------+\n",
      "|          null| 85465|\n",
      "|       nothing|698591|\n",
      "|     complaint|111612|\n",
      "|         issue| 59499|\n",
      "+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mentions.groupBy('classification').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "[Back to top](#top)\n",
    "## Step 2: Explore and Assess the Data <a class=\"anchor\" id=\"step-2\"></a>\n",
    "### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Lines\n",
    "Check null if any line row exists with ```line``` or ```regex``` with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "lines.filter(col('line').isNull() | col('regex').isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "classes = ['complaint', 'issue']\n",
    "mentions = mentions.filter(col('classification').isin(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions = mentions.withColumn('dt', to_timestamp(mentions.created_at, 'E MMM d HH:m:s Z y').alias('dt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "from pyspark import StorageLevel\n",
    "mentions = mentions.withColumn('full_text', \n",
    "                               when(~col('`extended_tweet_full_text`').isNull(), col('`extended_tweet_full_text`'))\n",
    "                               .otherwise(col('text'))).drop('text','`extended_tweet_full_text`') \\\n",
    "                               .persist(StorageLevel.MEMORY_ONLY_SER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "[Back to top](#top)\n",
    "## Step 3: Define the Data Model  <a class=\"anchor\" id=\"step-3\"></a>\n",
    "### 3.1 Conceptual Data Model\n",
    "The data we want to store is all messages that inform about any issue or complaint in a line or a station even if one message inform about an issue that affect two different lines. That the reason why the fact table is the inform fact, that can be a complaint or an issue. One tweet can inform about an issue that affect two lines (i.e.: a closed station and all lines that stops there). In other words, one tweet generates one or many \"inform fact\" records.\n",
    "\n",
    "![fact-dimension diagram](./img/class_diagram.png \"Fact-Dimension Diagram\")\n",
    "\n",
    "### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    " \n",
    "#### Line Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "[Back to top](#top)\n",
    "## Step 4: Run ETL to model the data<a class=\"anchor\" id=\"step-4\"></a>\n",
    "\n",
    "### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"./out\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line Dimension\n",
    "This dimension is based on the file content with all lines of Metro Madrid service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "lines_w = Window.orderBy('line_name')\n",
    "lines_dim = lines.withColumnRenamed('line','line_name').withColumn('line_id', row_number().over(lines_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lines dimension is persisted as parquet file in ```lines``` folder inside the ```OUTPUT_DIR``` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_file = os.path.join(OUTPUT_DIR, 'lines')\n",
    "lines_dim.write.mode('overwrite').parquet(lines_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Station Dimension\n",
    "Station dimensino is based on the file content with all stations of Metro de Madrid service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_w = Window.orderBy('station_name')\n",
    "stations_dim = stations.select(stations.station.alias('station_name'), 'regex').withColumn('station_id', row_number().over(station_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stations dimension is persisted as parquet file in ```stations``` folder inside the ```OUTPUT_DIR``` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_file = os.path.join(OUTPUT_DIR, 'stations')\n",
    "stations_dim.write.mode('overwrite').parquet(stations_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Class dimension\n",
    "This dimension contains all possible incident that can be detected on tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_df = mentions.select(col('classification').alias('class_name')).distinct()\n",
    "class_w = Window.orderBy('class_name')\n",
    "class_dim = class_df.withColumn('class_id', row_number().over(class_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class dimension is persisted as parquet file in ```classes``` folder inside the ```OUTPUT_DIR``` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_file = os.path.join(OUTPUT_DIR, 'classes')\n",
    "class_dim.write.mode('overwrite').parquet(class_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Date dimension\n",
    "This dimension is based on all date entries in tweet mentions. The ```create_at``` datetime is splitted in the following fields: ```year```, ```month```, ```day```, ```hour```, ```minute```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, quarter, dayofweek, dayofmonth, hour, minute\n",
    "\n",
    "date_w = Window.orderBy('year', 'month', 'day', 'hour', 'minute')\n",
    "\n",
    "date_dim = mentions.select(\n",
    "                        year(col('dt')).alias('year'),\n",
    "                        quarter(col('dt')).alias('quarter'),\n",
    "                        month(col('dt')).alias('month'),\n",
    "                        dayofmonth(col('dt')).alias('day'),\n",
    "                        dayofweek(col('dt')).alias('weekday'),\n",
    "                        hour(col('dt')).alias('hour'),\n",
    "                        minute(col('dt')).alias('minute')) \\\n",
    "                .dropDuplicates() \\\n",
    "                .withColumn('date_id', row_number().over(date_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date dimension is persisted as parquet file in ```date``` folder inside the ```OUTPUT_DIR``` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_file = os.path.join(OUTPUT_DIR, 'date')\n",
    "date_dim.write.mode('overwrite').parquet(date_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### User Dimension\n",
    "This step selects the columns and rename them:\n",
    "* ```user.id``` -> ```user_id```\n",
    "* ```user.name``` -> ```user_name```\n",
    "* ```user.screen_name``` -> ```screen_name```\n",
    "* ```user.description``` -> ```description```\n",
    "* ```user.profile_image_url``` -> ```profile_image_url```\n",
    "* ```user.profile_image_url_https``` -> ```profile_image_url_https```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dim = mentions.select(\n",
    "            'user_id',\n",
    "            'user_name',\n",
    "            col('user_screen_name').alias('screen_name'),\n",
    "            col('user_description').alias('description'),\n",
    "            col('user_profile_image_url').alias('profile_image_url'),\n",
    "            col('user_profile_image_url_https').alias('profile_image_url_https')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User dimension is persisted as parquet file in ```users``` folder inside the ```OUTPUT_DIR``` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_file = os.path.join(OUTPUT_DIR, 'users')\n",
    "user_dim.write.mode('overwrite').parquet(users_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Tweet Dimension\n",
    "\n",
    "To model this dimension, is join to the ```date``` dimension by year, month, day, hour and minute and get the following fields:\n",
    "* tweet_id\n",
    "* date_id\n",
    "* user_id\n",
    "* text\n",
    "* reply_tweet_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring\n",
    "\n",
    "tweet_dim = mentions.join(date_dim,\n",
    "              (year(mentions.dt) == date_dim.year) &\n",
    "              (month(mentions.dt) == date_dim.month) &\n",
    "              (dayofmonth(mentions.dt) == date_dim.day) &\n",
    "              (hour(mentions.dt) == date_dim.hour) &\n",
    "              (minute(mentions.dt) == date_dim.minute),\n",
    "              'inner'\n",
    "             ) \\\n",
    "        .select(col('id').alias('tweet_id'),\n",
    "                'date_id',\n",
    "                col('`user_id`').alias('user_id'),\n",
    "                col('full_text').alias('text'),\n",
    "                col('in_reply_to_status_id').alias('reply_tweet_id')\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweet dimension is persisted as parquet file in ```tweets``` folder inside the ```OUTPUT_DIR``` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_file = os.path.join(OUTPUT_DIR, 'tweets')\n",
    "tweet_dim.coalesce(2).write.mode('overwrite').parquet(tweets_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Fact table\n",
    "In the tweet text any station or line can be mentioned. So, we used the regex included in lines and stations dataset to search those lines and stations in tweet text.\n",
    "\n",
    "First, we need to extract two dictionaries with ids and regex for lines and stations. Then, we search every line and station regex in all tweet text, creating two new columns with an array that contains the ids of lines/stations mentioned in it.\n",
    "\n",
    "<img src=\"./img/regex_web.png\" style=\"height:450px;margin-left:auto;margin-right:auto;border:1px solid #888;\"/>\n",
    "\n",
    "First of all, 3 methods are defined:\n",
    "* ```gen_tags()```. This method returns an array with tags. This tags are based on the dictionary with regex and the given text.\n",
    "    If any regex is satisfied for the given text, the key of the dictionary is appended to the tag array.\n",
    "* ```gen_line_tags()```. This method returns the an array tag with all line_id found in the given text.\n",
    "* ```gen_station_tags()```. This method returns a tag array with all station_id found in the given text.\n",
    "\n",
    "Once those methods are defined, they are used to create 2 ```udf```:\n",
    "* ```gen_line_tags_udf```\n",
    "* ```gen_station_tags_udf```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import re\n",
    "\n",
    "lines_dict = {elem.line_id: elem.regex for elem in lines_dim.collect()}\n",
    "stations_dict = {elem.station_id: elem.regex for elem in stations_dim.collect()}\n",
    "\n",
    "def gen_tags(text,dicc):\n",
    "    \"\"\"\n",
    "    Returns an array with tags. This tags are based on the dictionary with regex and the given text.\n",
    "    If any regex is satisfied for the given text, the key of the dictionary is appended to the tag array.\n",
    "    \"\"\"\n",
    "    tags = []\n",
    "    for key, expr in dicc.items():\n",
    "        if re.search(expr, text, re.IGNORECASE):\n",
    "            tags.append(key)\n",
    "    return tags\n",
    "\n",
    "def gen_line_tags(text):\n",
    "    \"\"\"\n",
    "    Returns an tag array with all line_id found in the given text.\n",
    "    \"\"\"\n",
    "    return gen_tags(text, lines_dict)\n",
    "    \n",
    "def gen_station_tags(text):\n",
    "    \"\"\"\n",
    "    Returns a tag array with all station_id found in the given text.\n",
    "    \"\"\"\n",
    "    return gen_tags(text, stations_dict)\n",
    "\n",
    "gen_line_tags_udf = udf(gen_line_tags, ArrayType(StringType()))\n",
    "gen_station_tags_udf = udf(gen_station_tags, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those ```udf``` functions are used to generate to new array columns: ```lines``` constains all line_id founded in ```full_text``` column using ```gen_line_tags_udf```, ```stations``` performs ```gen_station_tags_udf``` to generate all station_id founded in ```full_text```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_aux = mentions.withColumn('lines', gen_line_tags_udf(col('full_text'))) \\\n",
    "                     .withColumn('stations', gen_station_tags_udf(col('full_text'))) \\\n",
    "                        .persist(StorageLevel.MEMORY_ONLY_SER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fact table has two array columns, we join to class dimentions to get class_id. After that, it does an ```explode_outer``` to get one row per array entry (in both columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode_outer\n",
    "\n",
    "fact_w = Window.orderBy('id')\n",
    "\n",
    "fact_df = fact_aux \\\n",
    "        .join(class_dim, col('classification') == class_dim.class_name, 'inner') \\\n",
    "        .withColumn('issue_id', row_number().over(fact_w)) \\\n",
    "        .select('issue_id', col('id').alias('tweet_id'), 'class_id', 'lines', 'stations') \\\n",
    "        .select('issue_id', 'tweet_id', explode_outer('lines').alias('line_id'), 'stations') \\\n",
    "        .select('issue_id', 'tweet_id', 'line_id', explode_outer('stations').alias('station_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fact table is persisted as parquet file in ```fact_table``` folder inside the ```OUTPUT_DIR``` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_file = os.path.join(OUTPUT_DIR, 'fact_table')\n",
    "fact_df.coalesce(4).write.mode('overwrite').parquet(fact_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Data Quality Checks\n",
    "Explain the data quality checks to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- screen_name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- profile_image_url: string (nullable = true)\n",
      " |-- profile_image_url_https: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_dim.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_df = fact_df.filter(~col('issue_id').isNull() | ~col('tweet_id').isNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Data dictionary \n",
    "\n",
    "#### Dimension Tables\n",
    "\n",
    "* **Line**\n",
    "    * ```line_id```\n",
    "        * ```Integer```\n",
    "        * Line identifier.\n",
    "    * ```line_name```\n",
    "        * ```String```\n",
    "        * Line name.\n",
    "    * ```regex```\n",
    "        * ```String```\n",
    "        * Regex to search the line in a text.\n",
    "* **Station**\n",
    "    * ```Integer```\n",
    "        * ```Long```\n",
    "        * Station identifier.\n",
    "    * ```station_name```\n",
    "        * ```String```\n",
    "        * Station name.\n",
    "    * ```regex```\n",
    "        * ```String```\n",
    "        * Regex to search the station in a text.\n",
    "* **Class**\n",
    "    * ```class_id```\n",
    "        * ```Integer```\n",
    "        * Station identifier.\n",
    "    * ```class_name```\n",
    "        * ```String```\n",
    "        * Station identifier.\n",
    "* **User**\n",
    "    * ```user_id```\n",
    "        * ```Long```\n",
    "        * User identifier.\n",
    "    * ```user_name```\n",
    "        * ```String```\n",
    "        * User name.\n",
    "    * ```screen_name```\n",
    "        * ```String```\n",
    "        * User unique string identifier.\n",
    "    * ```description```\n",
    "        * ```String```\n",
    "        * User description.\n",
    "    * ```profile_image_url```\n",
    "        * ```String```\n",
    "        * URL of profile image (HTTP protocol).\n",
    "    * ```profile_image_url_https```\n",
    "        * ```String```\n",
    "        * URL of profile image (HTTPS protocol).\n",
    "* **Date**\n",
    "    * ```date_id```\n",
    "        * ```Long```\n",
    "        * Date identifier.\n",
    "    * ```year```\n",
    "        * ```Integer```\n",
    "        * Year number (i.e.: 2019, 2020, 2021, ...).\n",
    "    * ```quarter```\n",
    "        * ```Integer```\n",
    "        * Quarter of the year (i.e.: 1, 2, ...).\n",
    "    * ```month```\n",
    "        * ```Integer```\n",
    "        * Month of the year as integer (i.e.: 1, 2, 3, 4, ...).\n",
    "    * ```weekday```\n",
    "        * ```Integer```\n",
    "        * Day of the week (Sunday=1, Monday=2, ..., Saturday=7).\n",
    "    * ```day```\n",
    "        * ```Integer```\n",
    "        * Day of the month (1, 2, 3, ...).\n",
    "    * ```hour```\n",
    "        * ```Integer```\n",
    "        * Hour in 24-hour format (i.e.: 0, 1, 2, ..., 12, 13, 14, ..., 22, 23).\n",
    "    * ```minute```\n",
    "        * ```Integer```\n",
    "        * Minute (i.e.: 0, 1, 2, 3, 4, ..., 59)\n",
    "* **Tweet**\n",
    "    * ```tweet_id```\n",
    "        * ```Long```\n",
    "        * Tweet identifier.\n",
    "    * ```date_id```\n",
    "        * ```Long```\n",
    "        * Date id when the tweet was created.\n",
    "    * ```user_id```\n",
    "        * ```Long```\n",
    "        * User id who is author of this tweet.\n",
    "    * ```text```\n",
    "        * ```String```\n",
    "        * Tweet text.\n",
    "    * ```reply_tweet_id```\n",
    "        * ```Long```\n",
    "        * If this tweet is a reply, this field references the tweet_id that this tweet is replying.\n",
    "\n",
    "#### Fact Table\n",
    "\n",
    "* **Inform**\n",
    "    * ```issue_id```\n",
    "        * ```Long```\n",
    "        * Inform identifier.\n",
    "    * ```tweet_id```\n",
    "        * ```Long```\n",
    "        * Tweet id that informs about a issue or complaint.\n",
    "    * ```line_id```\n",
    "        * ```Long```\n",
    "        * Service line id.\n",
    "    * ```station_id```\n",
    "        * ```Long```\n",
    "        * Station id.\n",
    "    * ```class_id```\n",
    "        * ```Long```\n",
    "        * If this tweet is a reply, this field references the tweet_id that this tweet is replying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "[Back to top](#top)\n",
    "### Step 5: Project Write Up <a class=\"anchor\" id=\"step-5\"></a>\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
